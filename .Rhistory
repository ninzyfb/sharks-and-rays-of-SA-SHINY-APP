##################### MPA OVERVIEW DATA
mpa_data_overview = readRDS(list.files(pattern = "mpa_data_overview.RDS",recursive=TRUE))
##################### EEZ DATA
eez_sa = st_read(list.files(pattern = "eez.shp",recursive = TRUE, full.names = TRUE))
##################### MASTER SHEET
# Path
#file_path <- list.files(pattern = "data_summary_master.xlsx", recursive=TRUE,full.names = TRUE)
# File
master = readxl::read_xlsx( list.files(pattern = "data_summary_master.xlsx", recursive=TRUE,full.names = TRUE))
# remove variables that i added previously
master = master[,c(1:4,12)]
##################### SPECIES PER MPA DATA
# File
species_overlapdata = read.csv(list.files(pattern = "species_permpa_byiusnandsdms.csv", recursive=TRUE,full.names = TRUE))
##################### RASTER DATA
# Read the data
#all_distributions = stack(list.files(path = "/home/nina/Dropbox/6-WILDOCEANS/1-ConservationPlan/wildoceans-scripts_github/Outputs/distribution_rasters/", pattern = "ensemblemean.tiff",full.names = TRUE))
#saveRDS(all_distributions,"all_distributions.RDS")
all_distributions = readRDS(list.files(pattern = "all_distributions.RDS",recursive = TRUE,full.names=TRUE))
##################### POLYGON DATA FROM IUCN
iucn_file_list = readRDS(list.files(pattern = "iucn_file_list.RDS",recursive = TRUE))
##################### PREPARE MASTER TABLE FOR FIRST PAGE
overlap_shortened = unique(species_overlapdata[,c(1,6,7)])
master$iucn = NA
master$iucn[which(master$SPECIES_SCIENTIFIC %in% overlap_shortened$SPECIES_SCIENTIFIC[overlap_shortened$type == "iucn"])] = "Yes"
master$iucn[which(is.na(master$iucn))] = "No"
master$sdm = NA
master$sdm[which(master$SPECIES_SCIENTIFIC %in% overlap_shortened$SPECIES_SCIENTIFIC[overlap_shortened$type == "sdm"])] = "Yes"
master$sdm[which(is.na(master$sdm))] = "No"
master = master %>%
mutate(ENDEMIC.STATUS = ifelse(ENDEMIC.STATUS == 1,"South Africa", ifelse(ENDEMIC.STATUS == 2,"Southern Africa", "Not endemic")))
##################### add some info to overlap data
species_overlapdata = left_join(species_overlapdata,master)
##################### Expert extents
load(list.files(pattern = "points.RData", recursive = TRUE, full.names = TRUE))
expert_extent = points
rm(points)
colnames(expert_extent)[1] = "SPECIES_SCIENTIFIC"
expert_extent$SPECIES_SCIENTIFIC = toupper(expert_extent$SPECIES_SCIENTIFIC)
expert_extent = st_as_sf(expert_extent)
expert_extent = left_join(overlap_shortened,expert_extent)
##################### EAST AND WEST BOUNDARIES PER SPECIES
boundaries = read.csv(list.files(pattern = "boundaries.csv",recursive = TRUE, full.names = TRUE))
##################### DATA POINTS PER HIGH RES GRID CELL
highres_gps = readRDS(list.files(pattern = "mostrecent_highres.RDS",recursive = TRUE, full.names = TRUE))
#####################  INITIAL MAP BOUNDARIES
# Define the initial map bounds (you can adjust these values)
initialBounds <- list(
lng1 = 17,
lat1 = -30,
lng2 = 33,
lat2 = -40
)
##################### MOST RECENT SIGHTING PER MPA
mostrecent_sighting = readRDS(list.files(pattern = "sightings_mostrecent.RDS",recursive = TRUE, full.names = TRUE))
mostrecent_sighting$SPECIES_SCIENTIFIC = as.character(mostrecent_sighting$SPECIES_SCIENTIFIC )
mostrecent_sighting = ungroup(mostrecent_sighting)
colnames(mostrecent_sighting)[which(colnames(mostrecent_sighting) == "SPECIES_SCIENTIFIC")] = "Scientific name"
mostrecent_sighting$`Scientific name` = str_to_sentence(mostrecent_sighting$`Scientific name`)
##################### DATA POINTS PER HIGH RES GRID CELL
lifehistory = read.csv(list.files(pattern = "lifehistory_parameters.csv",recursive = TRUE, full.names = TRUE))
##################### DATA POINTS PER HIGH RES GRID CELL
contours = readRDS(list.files(pattern = "contours.RDS",recursive = TRUE))
compiled_species_list = readRDS(list.files(pattern = "compiled_species_list",recursive = TRUE,full.names = TRUE))
compiled_species_list = readRDS(list.files(pattern = "compiled_species_list.RDS",recursive = TRUE,full.names = TRUE))
list.files(pattern = "compiled_species_list.RDS",recursive = TRUE,full.names = TRUE)
compiled_species_list = readRDS(list.files(pattern = "compiled-species_list.RDS",recursive = TRUE,full.names = TRUE))
runApp()
runApp()
runApp()
compiled_species_list[,"Record type"]
unique(compiled_species_list[,"Record type"])
datatable(compiled_species_list)
unique(shapefile_data_simple$CUR_NME)
temp = compiled_species_list %>% filter(CUR_NME == "Addo Elephant National Park Marine Protected Area")
# find columns with non empty values
options = unique(temp[,"Record type"])
options
naposition = which(is.na(options))
naposition
options = options[-naposition]
options
which
options
options = options[-naposition,]
options
datatable(temp, filter = "top", options = list(pageLength=10,autoWidth=TRUE,searching=FALSE))%>%
formatStyle(
columns = 'Record type',
target = 'row',
valueColumns = 'Record type',
backgroundColor = styleEqual(options,'lightgreen')
)
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp()
View(mostrecent_sighting)
View(highres_gps)
## LIST OF DATA PROVIDERS
highres_gps %>%
group_by(OWNER,DATATYPE)%>%
summarise()
## LIST OF DATA PROVIDERS
data_providers = highres_gps %>%
group_by(OWNER,DATATYPE)%>%
summarise()
View(data_providers)
write.csv(data_providers)
write.csv(data_providers,"data_providers.csv")
runApp()
runApp()
runApp()
# packages
library(tidyverse)
library(stringr)
library(sf)
library(readxl)
library(terra)
# list all dataset files
file_paths = list.files(pattern = ".csv",path = "/home/nina/Dropbox/shiny_webapp/code/data_objects/CLEANED_SPECIESDATA", full.names = TRUE)
# Initialize an empty data frame
combined_data <- data.frame()
# only keep column of interest (simple route for now)
headers_tokeep = c("LONGITUDE","LATITUDE","YEAR","MONTH","DAY","SPECIES_SCIENTIFIC","DATATYPE","OWNER")
# Loop through the CSV files and read them into data frames
for (file_path in file_paths) {
df <- read.csv(file_path, header = TRUE)
# turn column names to upper
colnames(df) = toupper(colnames(df))
# Identify missing columns and add them with NAs
missing_columns <- setdiff(headers_tokeep, colnames(df))
for (col in missing_columns) {
df[[col]] <- NA
}
rm(col,missing_columns)
# filter df to only keep headers
df = df[,which(colnames(df) %in% headers_tokeep)]
# add to one dataframe
combined_data <- rbind(combined_data, df)
# add to one dataframe
combined_data <- rbind(combined_data, df)
# check which one has error
print(file_path)
}
rm(df, file_path,file_paths,headers_tokeep)
# turn common name and scientific name to upper
combined_data$SPECIES_SCIENTIFIC = toupper(combined_data$SPECIES_SCIENTIFIC)
# some species names contain hidden characters
# this cleans all the hidden characters out (this takes a few minutes but is important)
for(i in 1:nrow(combined_data)){combined_data[i,"SPECIES_SCIENTIFIC"] = str_replace_all(combined_data[i,"SPECIES_SCIENTIFIC"], "\\s", " ")}
# change spelling mistakes
# load data sheet with species names to change
# this sheet contains common mistakes in scientific names
mistake_sheet = read_xlsx(list.files(pattern = "spelling_mistakes",recursive = TRUE,full.names = TRUE))
# change names to upper case
mistake_sheet$MISTAKE = toupper(mistake_sheet$MISTAKE)
mistake_sheet$CORRECTION = toupper(mistake_sheet$CORRECTION)
# fix in main dataset
# change any synonyms
for(i in 1:nrow(mistake_sheet)){
combined_data = combined_data %>%
mutate(SPECIES_SCIENTIFIC = ifelse(SPECIES_SCIENTIFIC == mistake_sheet$MISTAKE[i],mistake_sheet$CORRECTION[i],SPECIES_SCIENTIFIC))
}
rm(mistake_sheet,i)
# now simplify
# a lot of duplicates now as for some datasets there are the same observations multiple times a day but this isnt captured here
combined_data = unique(combined_data)
# split into genus and species
combined_data$GENUS = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,1]
combined_data$SPECIES = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,2]
# remove missing coordinates
combined_data_clean = combined_data[-which(is.na(combined_data$LONGITUDE)),]
# and coordinates with 0
combined_data_clean = combined_data_clean[-which(combined_data$LONGITUDE == 0),]
# look at datasets
unique(combined_data_clean$DATATYPE)
View(combined_data_clean)
# test with just a few datasets for now (BRUVS, ACOUSTIC ARRAY)
combined_data_clean = combined_data_clean %>%
filter(DATATYPE %in% c("ACOUSTIC","ACOUSTIC","ACOUSTICTAG","BRUVS","BRUV","UVC","UVC-S","UVC-F","RUV","LINEFISHING"))
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "BRUVS")] = "BRUV"
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "UVC-S")] = "UVC"
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "UVC-F")] = "UVC"
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "ACOUSTICTAG")] = "ACOUSTIC"
# look at datasets
unique(combined_data_clean$DATATYPE)
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean
# create specific date
boundaries$date = as.Date(paste0(boundaries$DAY,"/",boundaries$MONTH,"/",boundaries$YEAR), format = "%d/%m/%Y")
# get most recent west and east boundary
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary) %>%
group_by(SPECIES_SCIENTIFIC) %>%
filter(date == max(date))
boundaries_east = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(east_boundary = max(LONGITUDE))%>%
filter(LONGITUDE == east_boundary) %>%
group_by(SPECIES_SCIENTIFIC) %>%
filter(date == max(date))
# combine to one dataframe
boundaries = rbind(boundaries_east,boundaries_west)
# this file can now be saved as the boundaries file per species
boundaries = data.frame(boundaries) # ungroup
write.csv(boundaries,"boundaries.csv",row.names=FALSE)
rm(boundaries_east,boundaries_west)
# 2 - most recent sighting per grid cell
# grids of three resolutions
grids = readRDS(list.files(pattern = "list_rasters.RDS",recursive =TRUE,full.name=TRUE))
list.files(pattern = "list_rasters.RDS",recursive =TRUE,full.name=TRUE)
# 2 - most recent sighting per grid cell
# grids of three resolutions
grids = readRDS(list.files(pattern = "list_rasters.RDS",recursive =TRUE,full.name=TRUE))
?readRDS
# 2 - most recent sighting per grid cell
# grids of three resolutions
grids = terra::readRDS(list.files(pattern = "list_rasters.RDS",recursive =TRUE,full.name=TRUE))
# convert data to sf object
combined_data_clean_sf = st_as_sf(combined_data_clean, coords = c("LONGITUDE","LATITUDE"))
# assign occurrence points to grid cells at three resolutions
combined_data_clean_sf$reshigh = cellFromXY(grids[[1]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$resmedium = cellFromXY(grids[[2]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$reslow = cellFromXY(grids[[3]],st_coordinates(combined_data_clean_sf))
#remove geometry
combined_data_clean = st_drop_geometry(combined_data_clean_sf)
# add date
combined_data_clean$date = as.Date(paste0(combined_data_clean$DAY,"/",combined_data_clean$MONTH,"/",combined_data_clean$YEAR), format = "%d/%m/%Y")
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent) %>%
ungroup()
mostrecent_highres = unique(mostrecent_highres)
mostrecent_highres = mostrecent_highres %>%
group_by_at(vars(-OWNER,-DATATYPE)) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
# get your high, medium and low res polygons
# just do high res for now
polygons <- st_as_sf(rasterToPolygons(grids[[1]], dissolve = FALSE,na.rm=FALSE))
?rasterToPolygons
# get your high, medium and low res polygons
# just do high res for now
polygons <- st_as_sf(as.polygons(grids[[1]], dissolve = FALSE,na.rm=FALSE))
# get your high, medium and low res polygons
# just do high res for now
polygons <- st_as_sf(as.polygons(rast(grids[[1]]), dissolve = FALSE,na.rm=FALSE))
plot(polygons)
polygons$reshigh = 1:ncell(grids[[1]])
polygons$layer = NULL
# join your data
mostrecent_highres = left_join(mostrecent_highres,polygons)
View(mostrecent_highres)
View(combined_data_clean)
# convert data to sf object
combined_data_clean_sf = st_as_sf(combined_data_clean, coords = c("LONGITUDE","LATITUDE"))
View(combined_data_clean_sf)
# convert data to sf object
combined_data_clean_sf = st_as_sf(combined_data_clean, coords = c("LONGITUDE","LATITUDE"))
# remove missing coordinates
combined_data_clean = combined_data[-which(is.na(combined_data$LONGITUDE)),]
# and coordinates with 0
combined_data_clean = combined_data_clean[-which(combined_data$LONGITUDE == 0),]
# look at datasets
unique(combined_data_clean$DATATYPE)
# test with just a few datasets for now (BRUVS, ACOUSTIC ARRAY)
combined_data_clean = combined_data_clean %>%
filter(DATATYPE %in% c("ACOUSTIC","ACOUSTICTAG","BRUVS","BRUV","UVC","UVC-S","UVC-F","RUV","LINEFISHING"))
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "BRUVS")] = "BRUV"
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "UVC-S")] = "UVC"
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "UVC-F")] = "UVC"
combined_data_clean$DATATYPE[which(combined_data_clean$DATATYPE == "ACOUSTICTAG")] = "ACOUSTIC"
# convert data to sf object
combined_data_clean_sf = st_as_sf(combined_data_clean, coords = c("LONGITUDE","LATITUDE"))
View(combined_data_clean_sf)
# assign occurrence points to grid cells at three resolutions
combined_data_clean_sf$reshigh = cellFromXY(grids[[1]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$resmedium = cellFromXY(grids[[2]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$reslow = cellFromXY(grids[[3]],st_coordinates(combined_data_clean_sf))
#remove geometry
combined_data_clean = st_drop_geometry(combined_data_clean_sf)
# add date
combined_data_clean$date = as.Date(paste0(combined_data_clean$DAY,"/",combined_data_clean$MONTH,"/",combined_data_clean$YEAR), format = "%d/%m/%Y")
View(combined_data_clean)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent) %>%
ungroup()
View(mostrecent_highres)
mostrecent_highres = unique(mostrecent_highres)
mostrecent_highres = mostrecent_highres %>%
group_by_at(vars(-OWNER,-DATATYPE)) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
# get your high, medium and low res polygons
# just do high res for now
polygons <- st_as_sf(as.polygons(rast(grids[[1]]), dissolve = FALSE,na.rm=FALSE))
polygons$reshigh = 1:ncell(grids[[1]])
polygons$layer = NULL
# join your data
mostrecent_highres = left_join(mostrecent_highres,polygons)
# save shapefile (and as geojson for a test)
# and as .RDS R object
saveRDS(mostrecent_highres,"mostrecent_highres.RDS")
# generate data providers
combined_data_clean %>%
group_by(DATATYPE,OWNER)%>%
summarise()
View(combined_data)
# list all dataset files
file_paths = list.files(pattern = ".csv",path = "/home/nina/Dropbox/shiny_webapp/code/data_objects/CLEANED_SPECIESDATA", full.names = TRUE)
# Initialize an empty data frame
combined_data <- data.frame()
test = read.csv(file_paths[1])
View(test)
# generate data providers
data_providers = combined_data_clean %>%
group_by(DATATYPE,OWNER)%>%
summarise()
View(data_providers)
View(data_providers)
write_csv(data_providers.csv)
write_csv(data_providers,"data_providers.csv",row.names = FALSE)
write_csv(data_providers,"data_providers.csv")
library(readxl)
##################### list of data providers and institutions
providers = read.xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 1)
##################### list of data providers and institutions
providers = read-xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 1)
##################### list of data providers and institutions
providers = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 1)
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
View(providers)
shiny::runApp()
library(flextable)
runApp()
flextable(providers)
runApp()
flextable(providers)%>%
merge_v(j=c("Data type"))
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
width(width = 1.5)
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
width(width = 13)
runApp()
runApp()
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()%>%
bg(i = ~ "Data type" == "Acoustic tagging", bg = "Purple", part = "body")
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()%>%
bg(i = ~ "Data type" == "Acoustic tagging", bg = "Purple", part = "body")
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()%>%
bg(i = ~`Data type`=="Acoustic tagging",bg = "#606060",part = "all")
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()%>%
bg(i = ~`Data type`=="Acoustic tagging",bg = "#606060",part = "all")
flextable(providers)%>%
merge_v(j=c("Data type"))%>%
autofit()%>%
bg(i = ~`Data type`=="Acoustic tagging",bg = "#606060")
runApp()
providers = unique(providers)
institutions = unique(institutions)
runApp()
df = data.frame("A" = c("a","b","c","d"), "B" = c(1,2,3,4), "link" = c("www.a.com", "www.b.com", "www.c.com", "www.d.com"))
dt.ft <- flextable(data = df, col_keys = c("A", "B"))
dt.ft <- compose(x = dt.ft, j = 1, value = as_paragraph( hyperlink_text(x = A, url = link)))
dt.ft
View(institutions)
colnames(institutions)
dt.ft <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))
dt.ft
dt.ft <- compose(x = institutions, j = 1, value = as_paragraph( hyperlink_text(x = `Dataset affiliation (not necessarily current owner's affiliation)`, url = `Relevant affiliation link (if applicable)`)))
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))
dt.ft <- compose(x = institutions, j = 1, value = as_paragraph( hyperlink_text(x = `Dataset affiliation (not necessarily current owner's affiliation)`, url = `Relevant affiliation link (if applicable)`)))
dt.ft
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))
runApp()
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))
institutions <- compose(x = institutions, j = 1, value = as_paragraph( hyperlink_text(x = `Relevant affiliation link (if applicable)`, url = `Relevant affiliation link (if applicable)`)))
institutions
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions$Link = institutions$`Relevant affiliation link (if applicable)`
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)","Link"))
institutions <- compose(x = institutions, j = 1, value = as_paragraph( hyperlink_text(x = Link, url = `Relevant affiliation link (if applicable)`)))
institutions
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions$Link = institutions$`Relevant affiliation link (if applicable)`
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)","Link"))
institutions <- compose(x = institutions, j = 1, value = as_paragraph( hyperlink_text(x = `Relevant affiliation link (if applicable)`, url = Link)))
institutions
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions$Link = institutions$`Relevant affiliation link (if applicable)`
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)","Link"))
institutions <- compose(x = institutions, j = 1, value = as_paragraph( hyperlink_text(x = `Dataset affiliation (not necessarily current owner's affiliation)`, url = Link)))
institutions
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions$Link = institutions$`Relevant affiliation link (if applicable)`
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)","Link"))
institutions <- compose(x = institutions, j = 2, value = as_paragraph( hyperlink_text(x = `Relevant affiliation link (if applicable)`, url = Link)))
institutions
institutions[3,]
institutions$header
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))
institutions <- compose(x = institutions, j = 2, value = as_paragraph( hyperlink_text(x = `Relevant affiliation link (if applicable)`, url = `Relevant affiliation link (if applicable)`)))
institutions
institutions = read_xlsx(list.files(pattern = "data_providers_updated",recursive=TRUE,  full.names=TRUE),sheet = 2)
institutions = unique(institutions)
institutions <- flextable(data = institutions, col_keys = c("Dataset affiliation (not necessarily current owner's affiliation)", "Relevant affiliation link (if applicable)"))%>%
autofit()
institutions <- compose(x = institutions, j = 2, value = as_paragraph( hyperlink_text(x = `Relevant affiliation link (if applicable)`, url = `Relevant affiliation link (if applicable)`)))
institutions
runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(rsconnect)
deployApp()
?selectizeInput
runApp()
runApp()
#library(rsconnect)
deployApp()
library(rsconnect)
#library(rsconnect)
deployApp()
shiny::runApp()
shiny::runApp()
##################### IUCN COLORS
# Define a color palette for each IUCN Red List status
status_colors <- c(
"VU" = "rgba(255, 255, 0, 0.7)",     # Vulnerable
"LC" = "rgba(0, 128, 0, 0.7)",     # Least Concern
"EN" = "rgba(255, 165, 0, 0.7)",  # Endangered
"DD" = "rgba(128, 128, 128, 0.7)", # Data Deficient
"CR" = "rgba(255, 0, 0, 0.7)",     # Critically Endangered
"NT" = "#B0E57C"    # Near Threatened
)
##################### ENDEMIC COLORS
# Define a color palette for each IUCN Red List status
endemic_colors <- c(
"South Africa" = "#800080",   # Gold
"Southern Africa" = "#9370DB", # Silver
"Not endemic" = "#D8BFD8"     # Bronze
)
##################### ICONS
# Define custom icons
icon_blue <- makeIcon(
iconUrl = list.files(pattern = "boundary_cross.png",recursive=TRUE,full.names=TRUE),  # Replace with the path to your custom icon image
iconWidth = 32,  # Adjust the width and height as needed
iconHeight = 32
)
shiny::runApp()
runApp()
runApp()
# Load necessary libraries if any
library(shiny)
# Load UI function from ui.R
source("ui.R")
# Load UI function from ui.R
source("ui.R")
# Load UI function from ui.R
source("ui.R")
# Load UI function from ui.R
source("ui.R")
runApp()
