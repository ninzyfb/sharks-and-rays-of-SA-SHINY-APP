#fillOpacity = 0.7,
opacity = 1,
popup = paste0("Year: ",temp5$most_recent,"<Br>Data type: ",temp5$DATATYPE,"<Br>Owner: ",temp5$OWNER))
#color = "white",
#label = ~YEAR,  # Display the year as a label on the polygon
labelOptions = labelOptions(noHide = TRUE, textOnly = TRUE)
}
map1
map1 = map %>%
addPolygons(
data = temp5,
color = ~pal(most_recent),
weight = 0.5,
#fillOpacity = 0.7,
opacity = 0.5,
popup = paste0("Year: ",temp5$most_recent,"<Br>Data type: ",temp5$DATATYPE,"<Br>Owner: ",temp5$OWNER))
map1
temp5 = highres_gps[which(highres_gps$SPECIES_SCIENTIFIC == "CARCHARHINUS BRACHYURUS"),]
# list all dataset files
file_paths = list.files(pattern = ".csv",path = "/home/nina/Dropbox/shiny_webapp/code/data_objects/CLEANED_SPECIESDATA", full.names = TRUE)
# Initialize an empty data frame
combined_data <- data.frame()
# only keep column of interest (simple route for now)
headers_tokeep = c("LONGITUDE","LATITUDE","YEAR","MONTH","DAY","SPECIES_SCIENTIFIC","DATATYPE","OWNER")
# Loop through the CSV files and read them into data frames
for (file_path in file_paths) {
df <- read.csv(file_path, header = TRUE)
# turn column names to upper
colnames(df) = toupper(colnames(df))
# Identify missing columns and add them with NAs
missing_columns <- setdiff(headers_tokeep, colnames(df))
for (col in missing_columns) {
df[[col]] <- NA
}
rm(col,missing_columns)
# filter df to only keep headers
df = df[,which(colnames(df) %in% headers_tokeep)]
# add to one dataframe
combined_data <- rbind(combined_data, df)
# add to one dataframe
combined_data <- rbind(combined_data, df)
# check which one has error
print(file_path)
}
rm(df, file_path,file_paths,headers_tokeep)
# turn common name and scientific name to upper
combined_data$SPECIES_SCIENTIFIC = toupper(combined_data$SPECIES_SCIENTIFIC)
# change spelling mistakes
# load data sheet with species names to change
# this sheet contains common mistakes in scientific names
mistake_sheet = read_xlsx(list.files(pattern = "spelling_mistakes",recursive = TRUE,full.names = TRUE))
# change names to upper case
mistake_sheet$MISTAKE = toupper(mistake_sheet$MISTAKE)
mistake_sheet$CORRECTION = toupper(mistake_sheet$CORRECTION)
# fix in main dataset
# change any synonyms
for(i in 1:nrow(mistake_sheet)){
combined_data = combined_data %>%
mutate(SPECIES_SCIENTIFIC = ifelse(SPECIES_SCIENTIFIC == mistake_sheet$MISTAKE[i],mistake_sheet$CORRECTION[i],SPECIES_SCIENTIFIC))
}
rm(mistake_sheet,i)
# now simplify
# a lot of duplicates now as for some datasets there are the same observations multiple times a day but this isnt captured here
combined_data = unique(combined_data)
# split into genus and species
combined_data$GENUS = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,1]
combined_data$SPECIES = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,2]
# remove missing coordinates
combined_data_clean = combined_data[-which(is.na(combined_data$LONGITUDE)),]
# and coordinates with 0
combined_data_clean = combined_data_clean[-which(combined_data$LONGITUDE == 0),]
# test with just a few datasets for now (BRUVS, ACOUSTIC ARRAY)
combined_data_clean = combined_data_clean %>%
filter(DATATYPE %in% c("ACOUSTIC","BRUVS"))
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE),
south_boundary = max(LONGITUDE))%>%
filter(LONGITUDE == west_boundary | LONGITUDE == south_boundary)
# remove month and day to avoid duplicates
boundaries$MONTH = NULL
boundaries$DAY = NULL
boundaries = unique(boundaries)
# this file can now be saved as the boundaries file per species
boundaries = data.frame(boundaries) # ungroup
table(boundaries$SPECIES_SCIENTIFIC)
View(boundaries)
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean
# remove month and day to avoid duplicates
boundaries$MONTH = NULL
boundaries$DAY = NULL
boundaries = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE),
south_boundary = max(LONGITUDE))%>%
filter(LONGITUDE == west_boundary | LONGITUDE == south_boundary)
boundaries = unique(boundaries)
View(boundaries)
boundaries = unique(boundaries)
View(boundaries)
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary)
View(boundaries_west)
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & YEAR = max(YEAR))
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & YEAR == max(YEAR))
View(boundaries_west)
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & YEAR == max(YEAR))
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & YEAR == max(YEAR))
View(boundaries_west)
# create specific date
boundaries$date = as.Date(paste0(boundaries$DAY,boundaries$MONTH,boundaries$YEAR))
?as.Date
# create specific date
boundaries$date = as.Date(paste0(boundaries$DAY,boundaries$MONTH,boundaries$YEAR), format = "%d/%m/%Y")
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & YEAR == max(YEAR))
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & date == max(date))
View(boundaries_west)
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & date == max(date))
View(boundaries)
as.Date(paste0(boundaries$DAY,boundaries$MONTH,boundaries$YEAR), format = "%d/%m/%Y")
paste0(boundaries$DAY,boundaries$MONTH,boundaries$YEAR)
# create specific date
boundaries$date = as.Date(paste0(boundaries$DAY,"/",boundaries$MONTH,"/",boundaries$YEAR), format = "%d/%m/%Y")
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & date == max(date))
View(combined_data)
View(boundaries_west)
which(is.na(boundaries$date))
boundaries$date[which(is.na(boundaries$date))]
idx = which(is.na(boundaries$date))
boundaries$DAY[idx]
boundaries$MONTH[idx]
boundaries$YEAR[idx]
View(boundaries)
# packages
library(tidyverse)
library(stringr)
library(sf)
library(readxl)
library(terra)
# list all dataset files
file_paths = list.files(pattern = ".csv",path = "/home/nina/Dropbox/shiny_webapp/code/data_objects/CLEANED_SPECIESDATA", full.names = TRUE)
# Initialize an empty data frame
combined_data <- data.frame()
# only keep column of interest (simple route for now)
headers_tokeep = c("LONGITUDE","LATITUDE","YEAR","MONTH","DAY","SPECIES_SCIENTIFIC","DATATYPE","OWNER")
# Loop through the CSV files and read them into data frames
for (file_path in file_paths) {
df <- read.csv(file_path, header = TRUE)
# turn column names to upper
colnames(df) = toupper(colnames(df))
# Identify missing columns and add them with NAs
missing_columns <- setdiff(headers_tokeep, colnames(df))
for (col in missing_columns) {
df[[col]] <- NA
}
rm(col,missing_columns)
# filter df to only keep headers
df = df[,which(colnames(df) %in% headers_tokeep)]
# add to one dataframe
combined_data <- rbind(combined_data, df)
# add to one dataframe
combined_data <- rbind(combined_data, df)
# check which one has error
print(file_path)
}
rm(df, file_path,file_paths,headers_tokeep)
# turn common name and scientific name to upper
combined_data$SPECIES_SCIENTIFIC = toupper(combined_data$SPECIES_SCIENTIFIC)
# change spelling mistakes
# load data sheet with species names to change
# this sheet contains common mistakes in scientific names
mistake_sheet = read_xlsx(list.files(pattern = "spelling_mistakes",recursive = TRUE,full.names = TRUE))
# change names to upper case
mistake_sheet$MISTAKE = toupper(mistake_sheet$MISTAKE)
mistake_sheet$CORRECTION = toupper(mistake_sheet$CORRECTION)
# fix in main dataset
# change any synonyms
for(i in 1:nrow(mistake_sheet)){
combined_data = combined_data %>%
mutate(SPECIES_SCIENTIFIC = ifelse(SPECIES_SCIENTIFIC == mistake_sheet$MISTAKE[i],mistake_sheet$CORRECTION[i],SPECIES_SCIENTIFIC))
}
rm(mistake_sheet,i)
# now simplify
# a lot of duplicates now as for some datasets there are the same observations multiple times a day but this isnt captured here
combined_data = unique(combined_data)
# split into genus and species
combined_data$GENUS = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,1]
combined_data$SPECIES = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,2]
# remove missing coordinates
combined_data_clean = combined_data[-which(is.na(combined_data$LONGITUDE)),]
# and coordinates with 0
combined_data_clean = combined_data_clean[-which(combined_data$LONGITUDE == 0),]
# test with just a few datasets for now (BRUVS, ACOUSTIC ARRAY)
combined_data_clean = combined_data_clean %>%
filter(DATATYPE %in% c("ACOUSTIC","BRUVS"))
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean
# create specific date
boundaries$date = as.Date(paste0(boundaries$DAY,"/",boundaries$MONTH,"/",boundaries$YEAR), format = "%d/%m/%Y")
View(boundaries)
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary & date == max(date))
View(boundaries_west)
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary)
View(boundaries_west)
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary) %>%
group_by(SPECIES_SCIENTIFIC) %>%
filter(date == max(date))
View(boundaries_west)
boundaries_east = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(east_boundary = max(LONGITUDE))%>%
filter(LONGITUDE == east_boundary) %>%
group_by(SPECIES_SCIENTIFIC) %>%
filter(date == max(date))
View(boundaries_east)
# combine to one dataframe
rbind(boundaries_east,boundaries_west)
# combine to one dataframe
boundaries = rbind(boundaries_east,boundaries_west)
# simplify
boundaries = unique(boundaries)
# this file can now be saved as the boundaries file per species
boundaries = data.frame(boundaries) # ungroup
write.csv(boundaries,"boundaries.csv",row.names=FALSE)
rm(boundaries_east,boundaries_west)
# 2 - most recent sighting per grid cell
# grids of three resolutions
grids = readRDS(list.files(pattern = "list_rasters.RDS",recursive =TRUE,full.name=TRUE))
# convert data to sf object
combined_data_clean_sf = st_as_sf(combined_data_clean, coords = c("LONGITUDE","LATITUDE"))
# assign occurrence points to grid cells at three resolutions
combined_data_clean_sf$reshigh = cellFromXY(grids[[1]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$resmedium = cellFromXY(grids[[2]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$reslow = cellFromXY(grids[[3]],st_coordinates(combined_data_clean_sf))
#remove geometry
combined_data_clean = st_drop_geometry(combined_data_clean_sf)
# add date
combined_data_clean$date = as.Date(paste0(combined_data_clean$DAY,"/",combined_data_clean$MONTH,"/",combined_data_clean$YEAR), format = "%d/%m/%Y")
# do this by species
mostrecent_highres = combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date))%>%
filter(date == most_recent)
mostrecent_highres = unique(mostrecent_highres)
# do this by species
mostrecent_highres = combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date))%>%
filter(date == most_recent)
View(mostrecent_highres)
mostrecent_highres = unique(mostrecent_highres)
table(mostrecent_highres$reshigh)
View(mostrecent_highres)
df <- mostrecent_highres %>%
group_by(date) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
View(df)
df <- mostrecent_highres %>%
group_by_at(vars(-OWNER,-DATATYPE)) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
View(df)
# get most recent observation per cell
mostrecent_highres <- combined_data_clean %>%
group_by_at(vars(-OWNER,-DATATYPE)) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
mostrecent_highres = unique(mostrecent_highres)
View(mostrecent_highres)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by_at(vars(-OWNER,-DATATYPE)) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
# get your high, medium and low res polygons
# just do high res for now
polygons <- st_as_sf(rasterToPolygons(grids[[1]], dissolve = FALSE,na.rm=FALSE))
polygons$reshigh = 1:ncell(grids[[1]])
polygons$layer = NULL
# join your data
mostrecent_highres = left_join(mostrecent_highres,polygons)
# save shapefile (and as geojson for a test)
# and as .RDS R object
st_write(mostrecent_highres,"mostrecent_highres.shp")
st_write(mostrecent_highres,"mostrecent_highres.geojson") # test
saveRDS(mostrecent_highres,"mostrecent_highres.RDS")
View(mostrecent_highres)
shiny::runApp()
# list all dataset files
file_paths = list.files(pattern = ".csv",path = "/home/nina/Dropbox/shiny_webapp/code/data_objects/CLEANED_SPECIESDATA", full.names = TRUE)
# Initialize an empty data frame
combined_data <- data.frame()
# only keep column of interest (simple route for now)
headers_tokeep = c("LONGITUDE","LATITUDE","YEAR","MONTH","DAY","SPECIES_SCIENTIFIC","DATATYPE","OWNER")
# Loop through the CSV files and read them into data frames
for (file_path in file_paths) {
df <- read.csv(file_path, header = TRUE)
# turn column names to upper
colnames(df) = toupper(colnames(df))
# Identify missing columns and add them with NAs
missing_columns <- setdiff(headers_tokeep, colnames(df))
for (col in missing_columns) {
df[[col]] <- NA
}
rm(col,missing_columns)
# filter df to only keep headers
df = df[,which(colnames(df) %in% headers_tokeep)]
# add to one dataframe
combined_data <- rbind(combined_data, df)
# add to one dataframe
combined_data <- rbind(combined_data, df)
# check which one has error
print(file_path)
}
rm(df, file_path,file_paths,headers_tokeep)
# turn common name and scientific name to upper
combined_data$SPECIES_SCIENTIFIC = toupper(combined_data$SPECIES_SCIENTIFIC)
# change spelling mistakes
# load data sheet with species names to change
# this sheet contains common mistakes in scientific names
mistake_sheet = read_xlsx(list.files(pattern = "spelling_mistakes",recursive = TRUE,full.names = TRUE))
# change names to upper case
mistake_sheet$MISTAKE = toupper(mistake_sheet$MISTAKE)
mistake_sheet$CORRECTION = toupper(mistake_sheet$CORRECTION)
# fix in main dataset
# change any synonyms
for(i in 1:nrow(mistake_sheet)){
combined_data = combined_data %>%
mutate(SPECIES_SCIENTIFIC = ifelse(SPECIES_SCIENTIFIC == mistake_sheet$MISTAKE[i],mistake_sheet$CORRECTION[i],SPECIES_SCIENTIFIC))
}
rm(mistake_sheet,i)
# now simplify
# a lot of duplicates now as for some datasets there are the same observations multiple times a day but this isnt captured here
combined_data = unique(combined_data)
# split into genus and species
combined_data$GENUS = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,1]
combined_data$SPECIES = str_split(combined_data$SPECIES_SCIENTIFIC," ",simplify = TRUE)[,2]
# remove missing coordinates
combined_data_clean = combined_data[-which(is.na(combined_data$LONGITUDE)),]
# and coordinates with 0
combined_data_clean = combined_data_clean[-which(combined_data$LONGITUDE == 0),]
# test with just a few datasets for now (BRUVS, ACOUSTIC ARRAY)
combined_data_clean = combined_data_clean %>%
filter(DATATYPE %in% c("ACOUSTIC","BRUVS"))
# now for each species we want the following
# western and eastern boundary
# most recent sighting per grid cell
# most recent sighting per MPA
# 1 - W AND E BOUNDARY
# do this by species
boundaries = combined_data_clean
# create specific date
boundaries$date = as.Date(paste0(boundaries$DAY,"/",boundaries$MONTH,"/",boundaries$YEAR), format = "%d/%m/%Y")
# get most recent west and east boundary
boundaries_west = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(west_boundary = min(LONGITUDE))%>%
filter(LONGITUDE == west_boundary) %>%
group_by(SPECIES_SCIENTIFIC) %>%
filter(date == max(date))
boundaries_east = boundaries %>%
group_by(SPECIES_SCIENTIFIC) %>%
mutate(east_boundary = max(LONGITUDE))%>%
filter(LONGITUDE == east_boundary) %>%
group_by(SPECIES_SCIENTIFIC) %>%
filter(date == max(date))
# combine to one dataframe
boundaries = rbind(boundaries_east,boundaries_west)
# this file can now be saved as the boundaries file per species
boundaries = data.frame(boundaries) # ungroup
write.csv(boundaries,"boundaries.csv",row.names=FALSE)
rm(boundaries_east,boundaries_west)
# 1 - W AND E BOUNDARY
# 2 - most recent sighting per grid cell
# grids of three resolutions
grids = readRDS(list.files(pattern = "list_rasters.RDS",recursive =TRUE,full.name=TRUE))
# convert data to sf object
combined_data_clean_sf = st_as_sf(combined_data_clean, coords = c("LONGITUDE","LATITUDE"))
# assign occurrence points to grid cells at three resolutions
combined_data_clean_sf$reshigh = cellFromXY(grids[[1]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$resmedium = cellFromXY(grids[[2]],st_coordinates(combined_data_clean_sf))
combined_data_clean_sf$reslow = cellFromXY(grids[[3]],st_coordinates(combined_data_clean_sf))
#remove geometry
combined_data_clean = st_drop_geometry(combined_data_clean_sf)
# add date
combined_data_clean$date = as.Date(paste0(combined_data_clean$DAY,"/",combined_data_clean$MONTH,"/",combined_data_clean$YEAR), format = "%d/%m/%Y")
View(combined_data_clean)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,highres) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent)
combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,highres) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent)
View(mostrecent_highres)
mostrecent_highres = summarise(mostrecent_highres)
View(mostrecent_highres)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent)
View(mostrecent_highres)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent) %>%
ungroup()
mostrecent_highres = summarise(mostrecent_highres)
# do this by species
mostrecent_highres <- combined_data_clean %>%
group_by(SPECIES_SCIENTIFIC,reshigh) %>%
mutate(most_recent = max(date)) %>%
filter(date == most_recent) %>%
ungroup()
mostrecent_highres = unique(mostrecent_highres)
View(mostrecent_highres)
mostrecent_highres = mostrecent_highres %>%
group_by_at(vars(-OWNER,-DATATYPE)) %>%
summarize(OWNER = paste(OWNER, collapse = ", "),
DATATYPE = paste(DATATYPE,collapse = ","))
View(mostrecent_highres)
# get your high, medium and low res polygons
# just do high res for now
polygons <- st_as_sf(rasterToPolygons(grids[[1]], dissolve = FALSE,na.rm=FALSE))
polygons$reshigh = 1:ncell(grids[[1]])
polygons$layer = NULL
# join your data
mostrecent_highres = left_join(mostrecent_highres,polygons)
# save shapefile (and as geojson for a test)
# and as .RDS R object
st_write(mostrecent_highres,"mostrecent_highres.shp")
st_write(mostrecent_highres,"mostrecent_highres.geojson") # test
saveRDS(mostrecent_highres,"mostrecent_highres.RDS")
runApp()
##################### MASTER SHEET
# Path
file_path <- list.files(pattern = "data_summary_master.xlsx", recursive=TRUE,full.names = TRUE)
# File
master = readxl::read_xlsx(file_path)
rm(file_path)
# remove variables that i added previously
master = master[,c(1:4,12)]
View(master)
runApp()
install.packages('rsconnect')
rsconnect::setAccountInfo(name='sharks-rays-mpas',
token='FD4B55C87C3AF4D1D729FFF6C4F6E449',
secret='/abyH+jGSsBsZDHKSPlhHvKwcXVUD9P8RcJE29/M')
getwd()
rsconnect::deployApp("/home/nina/Dropbox/shiny_webapp/code")
rsconnect::deployApp("/home/nina/Dropbox/shiny_webapp/code")
shiny::runApp()
runApp()
runApp()
list.files(pattern = "data_summary_master.xlsx", recursive=TRUE,full.names = TRUE)
runApp()
